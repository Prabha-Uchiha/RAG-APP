Your provided code already uses **`microsoft/Phi-4-mini-instruct`** as the reader model and **`BAAI/bge-small-en-v1.5`** as the embedding model, as requested, and implements the secure retrieval of the Hugging Face token from `secrets.toml`.

However, you should make one **critical adjustment** to ensure the Phi-4 model (which may have a very large context window, e.g., 128K tokens) can actually use the long context provided by the RAG system. You need to adjust the way the **prompt is constructed** and **injected** into the pipeline to utilize the full context efficiently.

Here's the minimal, necessary change in the `answer_with_rag` method to handle the prompt better:

## üîÑ Code Review and Improvement

### 1\. Change in `main()` function:

I noticed you didn't have the `disabled` state on the buttons in your latest code snippet, which can lead to errors if models aren't loaded. I've updated the `main` function to include a session state flag to track model loading status for a better user experience.

### 2\. The Core Change: Passing `hf_token` to `initialize_models`

Your current `initialize_models` function expects the token to be read from `st.secrets` internally and doesn't take an argument:

```python
    @st.cache_resource
    def initialize_models(_self):
        # ... token reading logic ...
        hf_token = st.secrets.get("huggingface", {}).get("token")
        # ... rest of the function
```

Since the token is read *inside* the function, `st.cache_resource` might not recognize when the token changes, causing potential issues if you were to switch secrets. While this is acceptable for deployment, if you intend to switch to a **user input field** (as discussed in a previous prompt), the method signature must change.

**For now, I will keep the `secrets.toml` approach (as it is what you provided) but ensure the logic for model loading and error handling is robust.**

-----

## ‚úÖ Full, Corrected `app.py` for Phi-4 Mini RAG

This version uses **Phi-4 Mini-Instruct**, **BGE embeddings**, and **securely loads the token** from `secrets.toml`, while ensuring the UI elements are correctly disabled until the models are initialized.

````python
import streamlit as st
import pandas as pd
import numpy as np
from typing import List, Optional, Tuple
import tempfile
import os
from pathlib import Path
import torch

# Core RAG imports
from langchain_core.documents import Document as LangchainDocument
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.vectorstores.utils import DistanceStrategy
from langchain_huggingface import HuggingFaceEmbeddings
# Import the necessary function for login (for downloading gated models)
from huggingface_hub import login 
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline

# Document processing imports
import fitz
import docx

# Configuration
EMBEDDING_MODEL_NAME = "BAAI/bge-small-en-v1.5"
READER_MODEL_NAME = "microsoft/Phi-4-mini-instruct"

MARKDOWN_SEPARATORS = [
    "\n#{1,6} ",
    "```\n",
    "\n***+\n",
    "\n---+\n",
    "\n___+\n",
    "\n\n",
    "\n",
    " ",
    "",
]

class RAGSystem:
    def __init__(self):
        self.embedding_model = None
        self.vector_database = None
        self.reader_llm = None
        self.tokenizer = None
        self.prompt_template = None
        self.documents = []
        
    @st.cache_resource
    def initialize_models(_self):
        """Initialize embedding model (BGE) and reader LLM (Phi-4 Mini) using a secure token."""
        try:
            # ----------------------------------------------------
            # 1. SECURE TOKEN HANDLING (Reads the token from secrets.toml)
            # ----------------------------------------------------
            hf_token = st.secrets.get("huggingface", {}).get("token")
            if not hf_token:
                st.error("Hugging Face token not found in Streamlit Secrets. Cannot load models.")
                return False

            # Log in using the token for gated model access (Phi-4 is gated)
            login(token=hf_token, add_to_git_credential=False)
            
            # 2. Initialize BGE Embedding model
            _self.embedding_model = HuggingFaceEmbeddings(
                model_name=EMBEDDING_MODEL_NAME,
                model_kwargs={"device": "cpu"},
                encode_kwargs={"normalize_embeddings": True}
            )
            
            # 3. Initialize Tokenizer for Phi-4
            _self.tokenizer = AutoTokenizer.from_pretrained(
                READER_MODEL_NAME, 
                trust_remote_code=True,
                token=hf_token # Pass token explicitly to the tokenizer 
            )
            if _self.tokenizer.pad_token is None:
                _self.tokenizer.pad_token = _self.tokenizer.eos_token
            
            # 4. Configure and Load Phi-4 Model (4-bit quantization)
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_compute_dtype=torch.bfloat16,
            )
            
            model = AutoModelForCausalLM.from_pretrained(
                READER_MODEL_NAME, 
                quantization_config=bnb_config,
                device_map="auto",
                trust_remote_code=True,
                token=hf_token # Pass token explicitly to the model loader
            )
            
            _self.reader_llm = pipeline(
                "text-generation",
                model=model,
                tokenizer=_self.tokenizer,
                do_sample=True,
                temperature=0.2,
                repetition_penalty=1.1,
                return_full_text=False,
                max_new_tokens=500,
            )
            
            # 5. Create prompt template
            # System message instructs the LLM on its role
            system_message = "Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide the number of the source document when relevant. If the answer cannot be deduced from the context, do not give an answer."

            # The Chat Format Template structures the prompt for instruction-tuned models like Phi-4
            prompt_in_chat_format = [
                {"role": "system", "content": system_message},
                {"role": "user", "content": "Context:\n{context}\n---\nNow here is the question you need to answer.\nQuestion: {question}"},
            ]
            
            _self.prompt_template = _self.tokenizer.apply_chat_template(
                prompt_in_chat_format, 
                tokenize=False, 
                add_generation_prompt=True
            )
            
            return True
        except Exception as e:
            st.error(f"Error initializing models: {str(e)}")
            return False
    
    # --- Document Processing Methods (Unchanged) ---
    def extract_text_from_file(self, uploaded_file) -> str:
        try:
            if uploaded_file.type == "text/plain":
                return str(uploaded_file.read(), "utf-8")
            elif uploaded_file.type == "application/pdf":
                with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
                    tmp_file.write(uploaded_file.getvalue())
                    tmp_file_path = tmp_file.name
                
                doc = fitz.open(tmp_file_path)
                text = ""
                for page in doc:
                    text += page.get_text()
                doc.close()
                os.unlink(tmp_file_path)
                return text
            elif uploaded_file.type == "application/vnd.openxmlformats-officedocument.wordprocessingml.document":
                with tempfile.NamedTemporaryFile(delete=False, suffix=".docx") as tmp_file:
                    tmp_file.write(uploaded_file.getvalue())
                    tmp_file_path = tmp_file.name
                
                doc = docx.Document(tmp_file_path)
                text = ""
                for paragraph in doc.paragraphs:
                    text += paragraph.text + "\n"
                os.unlink(tmp_file_path)
                return text
            else:
                st.error(f"Unsupported file type: {uploaded_file.type}")
                return ""
        except Exception as e:
            st.error(f"Error extracting text: {str(e)}")
            return ""
    
    def split_documents(self, chunk_size: int, documents: List[LangchainDocument]) -> List[LangchainDocument]:
        try:
            tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL_NAME) 
            text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(
                tokenizer,
                chunk_size=chunk_size,
                chunk_overlap=int(chunk_size / 10),
                add_start_index=True,
                strip_whitespace=True,
                separators=MARKDOWN_SEPARATORS,
            )
            
            docs_processed = []
            for doc in documents:
                docs_processed += text_splitter.split_documents([doc])
            
            unique_texts = {}
            docs_processed_unique = []
            for doc in docs_processed:
                if doc.page_content not in unique_texts:
                    unique_texts[doc.page_content] = True
                    docs_processed_unique.append(doc)
            
            return docs_processed_unique
        except Exception as e:
            st.error(f"Error splitting documents: {str(e)}")
            return []
    
    def create_vector_database(self, docs_processed: List[LangchainDocument]):
        try:
            if not self.embedding_model:
                st.error("Embedding model not initialized")
                return False
            
            self.vector_database = FAISS.from_documents(
                docs_processed, 
                self.embedding_model, 
                distance_strategy=DistanceStrategy.COSINE
            )
            return True
        except Exception as e:
            st.error(f"Error creating vector database: {str(e)}")
            return False
    
    def answer_with_rag(self, question: str, num_retrieved_docs: int = 5) -> Tuple[str, List[str]]:
        try:
            if not self.vector_database:
                return "No knowledge base available. Please upload documents first.", []
            
            relevant_docs = self.vector_database.similarity_search(
                query=question, 
                k=num_retrieved_docs
            )
            
            relevant_docs_text = []
            context_parts = []
            for i, doc in enumerate(relevant_docs):
                relevant_docs_text.append(doc.page_content)
                context_parts.append(f"Document {i} (Source: {doc.metadata.get('source', 'Unknown')}):::\n{doc.page_content}\n\n")
            
            context = "\nExtracted documents:\n" + "".join(context_parts)
            
            final_prompt = self.prompt_template.format(context=context, question=question)

            if self.reader_llm:
                response = self.reader_llm(final_prompt)
                answer = response[0]["generated_text"].strip()
            else:
                answer = "Reader model not available. Showing retrieved context only."
            
            return answer, relevant_docs_text
        except Exception as e:
            return f"Error generating answer: {str(e)}", []

def get_rag_system() -> 'RAGSystem':
    if 'rag_system' not in st.session_state:
        st.session_state['rag_system'] = RAGSystem()
    return st.session_state['rag_system']

def main():
    st.set_page_config(
        page_title="Phi-4 Mini RAG System",
        page_icon="üß†",
        layout="wide"
    )
    
    st.title("üß† Phi-4 Mini & BGE Advanced RAG System")
    st.markdown("Powered by **Microsoft Phi-4 Mini (Instruct)** and **BAAI/bge-small-en-v1.5** embeddings.")
    
    rag_system = get_rag_system()
    
    # Initialize session state for tracking model status
    if 'models_loaded' not in st.session_state:
        st.session_state.models_loaded = False
    
    # Check if models are loaded for controlling other UI elements
    models_loaded = st.session_state.get('models_loaded', False)

    with st.sidebar:
        st.header("üõ†Ô∏è Configuration")
        
        st.subheader("Model Setup")
        st.info("Ensure your Hugging Face token is set in `.streamlit/secrets.toml` to load Phi-4.")
        
        # Initialization Button
        if st.button("Initialize Models", type="primary", disabled=models_loaded):
            with st.spinner("Loading Phi-4 and BGE models..."):
                success = rag_system.initialize_models() 
                if success:
                    st.session_state.models_loaded = True
                    st.success("Models loaded successfully! (Token loaded from secrets)")
                else:
                    st.session_state.models_loaded = False
                    st.error("Failed to load models. Check token or access permissions.")
        
        if models_loaded:
            st.success("Models Initialized!")
        
        st.subheader("Chunking Parameters")
        chunk_size = st.slider("Chunk Size (tokens)", 100, 1000, 512, disabled=not models_loaded)
        
        st.subheader("Retrieval Parameters")
        num_docs = st.slider("Number of documents to retrieve", 1, 10, 5, disabled=not models_loaded)
    
    col1, col2 = st.columns([1, 1])
    
    with col1:
        st.header("üìÑ Document Upload & Processing")
        
        if not models_loaded:
            st.warning("Initialize models first to enable document processing.")

        uploaded_files = st.file_uploader(
            "Upload documents",
            accept_multiple_files=True,
            type=['txt', 'pdf', 'docx'],
            help="Upload PDF, DOCX, or TXT files to build your knowledge base",
            disabled=not models_loaded
        )
        
        if uploaded_files:
            st.subheader("Uploaded Files")
            for file in uploaded_files:
                st.write(f"üìÅ {file.name} ({file.type})")
        
        # Processing Button
        if st.button("Process Documents", type="primary", disabled=not models_loaded or not uploaded_files):
            with st.spinner("Processing documents..."):
                raw_documents = []
                for file in uploaded_files:
                    text = rag_system.extract_text_from_file(file)
                    if text:
                        raw_documents.append(
                            LangchainDocument(
                                page_content=text,
                                metadata={"source": file.name}
                            )
                        )
                
                if raw_documents:
                    docs_processed = rag_system.split_documents(chunk_size, raw_documents)
                    st.session_state.docs_processed = docs_processed
                    
                    if rag_system.embedding_model:
                        success = rag_system.create_vector_database(docs_processed)
                        if success:
                            st.session_state.knowledge_base_ready = True
                            st.success(f"Successfully processed {len(docs_processed)} chunks from {len(uploaded_files)} documents!")
                            
                            st.subheader("Processing Statistics")
                            st.metric("Total Chunks", len(docs_processed))
                            st.metric("Average Chunk Length", 
                                    int(np.mean([len(doc.page_content) for doc in docs_processed])))
                        else:
                            st.error("Failed to create vector database")
                    else:
                        st.warning("Models not initialized.")
                else:
                    st.error("No valid documents found")
        
        if 'docs_processed' in st.session_state and st.session_state.docs_processed:
            st.subheader("Document Chunks Preview")
            chunk_idx = st.selectbox(
                "Select chunk to preview", 
                range(len(st.session_state.docs_processed)),
                format_func=lambda x: f"Chunk {x+1}"
            )
            
            selected_chunk = st.session_state.docs_processed[chunk_idx]
            st.text_area(
                f"Chunk {chunk_idx + 1} Content",
                selected_chunk.page_content,
                height=200,
                disabled=True
            )
            st.json(selected_chunk.metadata)
    
    with col2:
        st.header("‚ùì Query Interface")
        
        knowledge_base_ready = st.session_state.get('knowledge_base_ready', False)
        
        if not models_loaded:
            st.info("Initialize models in the sidebar first.")
        elif not knowledge_base_ready:
            st.info("Please upload and process documents first to enable querying.")
        else:
            st.success("Knowledge base is ready! Ask your questions below.")
        
        user_query = st.text_input(
            "Enter your question:",
            placeholder="What is the key takeaway about Phi-4's memory capacity?",
            disabled=not knowledge_base_ready
        )
        
        if st.button("Ask Question", disabled=not knowledge_base_ready or not user_query):
            if user_query and rag_system.vector_database:
                with st.spinner("Searching and generating answer..."):
                    answer, retrieved_docs = rag_system.answer_with_rag(
                        user_query, 
                        num_retrieved_docs=num_docs
                    )
                    
                    st.subheader("Answer")
                    st.markdown(answer)
                    
                    st.subheader("Retrieved Documents")
                    for i, doc in enumerate(retrieved_docs):
                        with st.expander(f"Document {i+1}"):
                            st.text_area(
                                "Content", 
                                doc, 
                                height=150, 
                                disabled=True,
                                key=f"doc_{i}"
                            )
        
        if 'query_history' not in st.session_state:
            st.session_state.query_history = []
        
        if user_query and st.button("Save Query", disabled=not user_query):
            st.session_state.query_history.append(user_query)
        
        if st.session_state.query_history:
            st.subheader("Query History")
            for i, query in enumerate(reversed(st.session_state.query_history[-5:])):
                st.text(f"{len(st.session_state.query_history)-i}. {query}")
    
    st.markdown("---")
    st.markdown("""
    ## üöÄ Setup Instructions:
    
    1. **Token Setup**: Create `.streamlit/secrets.toml` in your project root and add your `huggingface` token:
    ```toml
    # .streamlit/secrets.toml
    [huggingface]
    token = "hf_YOUR_ACTUAL_TOKEN_HERE"
    ```
    2. **Dependencies**: Install the required packages.

    ```bash
    pip install streamlit pandas numpy torch transformers accelerate bitsandbytes
    pip install langchain-core langchain-text-splitters langchain-community langchain-huggingface huggingface-hub
    pip install faiss-cpu sentence-transformers PyMuPDF python-docx
    ```
    """)

if __name__ == "__main__":
    main()
````